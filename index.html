
<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" href="https://vanillacss.com/vanilla.css">
    <style>
        body{margin:0 auto;max-width:50rem;}
        @media(max-width:50rem) {
            body {
                padding: 10px;
            }
        }
    </style>
</head>
<body>
<div id="top" class="page" role="document">
\(
  \require{ams}
\DeclareMathOperator*{\softmax}{softmax}
\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\name}[1]{\mathsf{#1}}
\newcommand{\ndot}[1]{\mathbin{\mathop{\odot}_{\name{#1}}}}
\newcommand{\nsum}[1]{\mathbin{\mathop{\sum}_{\name{#1}}}}
\newcommand{\ntup}[2]{\name{#1}:#2}
\newcommand{\nfun}[2]{\underset{\name{#1}}{#2}}
\)

  <header>
 <h1> Named Tensor Notation<h1>
   
   </header>
  <h2>Summary</h2>
   Matrix notation is concise, but often not helpful for explaining 
   new concepts. Readers often get confused aligning dimensions 
   to concepts and resolving unnecessary transpositional ambiguities. 
   This page provides some examples of <i>named tensor notation</i>, a proposal 
   for writing tensor calculations. Named tensor notation is based on three ideas
   1) unordered named dimensions, 2) broadcasting, and 3) named reduction syntax. 
   All tensor dimensions must have an explicit name, and 
   critically, all operations must use these name and are not use dimension-ordering conventions. 
   This means matrix multiplication, transpose, and friends are gone entirely. Instead we write "matrix 
   multiply" as a broadcasted multiply and a sum over the named dimension \( \ndot{dim} \). 
   Names can also appear in sums, expectations, and parameterize arbitrary functions like softmax. 
   
   For the full formal specification of the notation see http://github.com/namedtensor/spec.  
   
   Here are some examples in action. 
   
  <h2> Attention</h2>  
\(
  \begin{align*} 
Q &\in \reals^{\ntup{key}{d_v},\ntup{time'}{n}}\\
K &\in \reals^{\ntup{head}{h},\ntup{key}{d_k}, \ntup{time}{n}}\\
V &\in \reals^{\ntup{head}{h}, \ntup{val}{d_v}, \ntup{time}{n}}\\
\text{attention}(Q, K, V) &=  \nfun{time}{\softmax} \left( \frac{Q \ndot{key} K }{\sqrt{d_k}} \right) \ndot{time} V 
\end{align*}
\)  
  
  <h2>MLP</h2>
  \(
   \begin{align*} 
V &\in \reals^{\ntup{output}{o}, \ntup{hidden}{h}},\ c\in \reals^{\ntup{output}{o}} \\
W &\in \reals^{{\ntup{hidden}{h}, \ntup{in}{i}}}, \ b \in \reals^{\ntup{hidden}{h}} \\
X &\in \reals^{{\ntup{batch}{b}, \ntup{in}{i}}} \\
\text{MLP}(X; V, W,b, c) &= \sigma \left( V \ndot{hidden} \sigma \left( W \ndot{in} X + b \right) + c \right)  
\end{align*}
   \)

<h2>NN Norms</h2>

   \(
   \begin{align*} 
X &\in \reals^{{\ntup{batch}{b}, \ntup{channel}{c}, \ntup{hidden}{h}}}\\
\gamma, \beta &\in \reals^{{\ntup{batch}{b}}} \\
\text{batchnorm}(X; \gamma, \beta) &= \frac{X - \nfun{batch}{\mathbb{E}}[X]}{\sqrt{\nfun{batch}{\text{var}}(X)} + \epsilon } \odot \gamma + \beta
\end{align*}

   \)
   </div>
   
</body>
