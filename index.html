
<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" href="https://vanillacss.com/vanilla.css">
    <style>
        body{margin:0 auto;max-width:50rem;}
        @media(max-width:50rem) {
            body {
                padding: 10px;
            }
        }
    </style>
</head>
<body>
<div id="top" class="page" role="document">
\(
  \require{ams}
\DeclareMathOperator*{\softmax}{softmax}
\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\name}[1]{\mathsf{#1}}
\newcommand{\ndot}[1]{\mathbin{\mathop{\odot}_{\name{#1}}}}
\newcommand{\nsum}[1]{\mathbin{\mathop{\sum}_{\name{#1}}}}
\newcommand{\ntup}[2]{\name{#1}:#2}
\newcommand{\nfun}[2]{\underset{\name{#1}}{#2}}
\)

  <header>
 <h1> Named Tensor Notation<h1>
   
   </header>
  <h2>Summary</h2>
   Matrix notation is concise, but not helpful for explaining 
   new ideas. Readers often get confused aligning dimensions 
   to concepts and resolving transpositional ambiguities. 
   <i>Named tensor notation</i> is a proposal 
   for writing tensor calculations with 1) unordered dimensions, 2) named broadcasting, and 3) named reduction. 
   All tensor dimensions must have an explicit name and cannot accessed by convention. 
   This means no matrix multiplication or transpose at all. To perform a matrix 
   dot, we do a broadcasted multiply and a sum over the named dimension \( \ndot{dim} \). 
   Names can also appear in sums, expectations, and parameterize arbitrary functions like softmax. 
   
   <br> 
   For the full formal specification of the notation see http://github.com/namedtensor/spec.  
   
   <h2>Examples</h2>
  <h3> Attention</h3>  
\(
  \begin{align*} 
Q &\in \reals^{\ntup{key}{d_v},\ntup{time'}{n}}\\
K &\in \reals^{\ntup{head}{h},\ntup{key}{d_k}, \ntup{time}{n}}\\
V &\in \reals^{\ntup{head}{h}, \ntup{val}{d_v}, \ntup{time}{n}}\\
\text{attention}(Q, K, V) &=  \nfun{time}{\softmax} \left( \frac{Q \ndot{key} K }{\sqrt{d_k}} \right) \ndot{time} V 
\end{align*}
\)  
  
  <h3>MLP</h3>
  \(
   \begin{align*} 
V &\in \reals^{\ntup{output}{o}, \ntup{hidden}{h}},\ c\in \reals^{\ntup{output}{o}} \\
W &\in \reals^{{\ntup{hidden}{h}, \ntup{in}{i}}}, \ b \in \reals^{\ntup{hidden}{h}} \\
X &\in \reals^{{\ntup{batch}{b}, \ntup{in}{i}}} \\
\text{MLP}(X; V, W,b, c) &= \sigma \left( V \ndot{hidden} \sigma \left( W \ndot{in} X + b \right) + c \right)  
\end{align*}
   \)

<h3>NN Norms</h3>

   \(
   \begin{align*} 
X &\in \reals^{{\ntup{batch}{b}, \ntup{channel}{c}, \ntup{hidden}{h}}}\\
\gamma, \beta &\in \reals^{{\ntup{batch}{b}}} \\
\text{batchnorm}(X; \gamma, \beta) &= \frac{X - \nfun{batch}{\mathbb{E}}[X]}{\sqrt{\nfun{batch}{\text{var}}(X)} + \epsilon } \odot \gamma + \beta
\end{align*}

   \)
   </div>
   
</body>
