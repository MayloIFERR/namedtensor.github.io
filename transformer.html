<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Named Tensors: Transformer</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="/usr/share/javascript/mathjax/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://vanillacss.com/vanilla.css">
      <style>
          body{margin:0 auto;max-width:50rem;}
          @media(max-width:50rem) {
              body {
                  padding: 10px;
              }
          }
      </style>
  
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <meta name="author" content="David Chiang and Sasha Rush" />
    <title>Named Tensor Notation</title>
    <style type="text/css">
        code{white-space: pre-wrap;}
        span.smallcaps{font-variant: small-caps;}
        span.underline{text-decoration: underline;}
        div.column{display: inline-block; vertical-align: top; width: 50%;}
    </style>
    <script src="/usr/share/javascript/mathjax/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  <div style="display:none">
  \(
    \require{ams}
    \DeclareMathOperator*{\softmax}{softmax}
    \DeclareMathOperator{\tupleshape}{ind}
    \newcommand{\ensuremath}[1]{#1}
  \)
  </div>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Named Tensors: Transformer</h1>
</header>
<h4 id="ffn">FFN</h4>
<p><span class="math display">\[\begin{aligned}
X &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]}} \\
W^1 &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]}, \ensuremath{\ensuremath{\mathsf{hid}}[d_{\text{ff}}]}} &amp; 
b^1 &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{hid}}[d_{\text{ff}}]}} \\
W^2 &amp; \in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{hid}}[d_{\text{ff}}]} \times \ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]}} &amp; b^2 &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{hid}}[d_{\text{ff}}]}} \\
\text{FFN}(X; W, b) &amp;=  W^2 \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{hid}}}} \text{ReLU}(W^1 \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{emb}}}} X + b^1) + b^2\end{aligned}\]</span></p>
<h4 id="masked-attention">Masked Attention</h4>
<p><span class="math display">\[\begin{aligned}
Q &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{key}}[d_k]} \times \ensuremath{\ensuremath{\mathsf{seq&#39;}}[n]}} \times K \in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{key}}[d_k]} \times \ensuremath{\ensuremath{\mathsf{seq}}[n]}}\\
V &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{seq}}[n]} \times \ensuremath{\ensuremath{\mathsf{val}}[d_v]}fg
},
M \in \mathbb{R}^{\ensuremath{\mathsf{seq}}, \ensuremath{\mathsf{seq&#39;}}}\\
\text{att}(Q, K, V, M) &amp;=  V \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{seq}}}} \mathop{\mathrm{softmax}}\limits_{\ensuremath{\mathsf{seq}}} \left( \frac{\displaystyle Q \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{key}}}} K }{\sqrt{d_k}} + M \right) \end{aligned}\]</span></p>
<h4 id="multiheaded-self-attention">Multiheaded Self Attention</h4>
<p><span class="math display">\[\begin{aligned}
  W^Q &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{head}}[h]} \times \ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]} \times \ensuremath{\ensuremath{\mathsf{key}}[d_k]}} \\
  W^K &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{head}}[h]} \times \ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]} \times \ensuremath{\ensuremath{\mathsf{key}}[d_k]}} \\
  W^V &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{head}}[h]} \times \ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]} \times \ensuremath{\ensuremath{\mathsf{val}}[d_k]}} \\
  W^O &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{head}}[h]} \times \ensuremath{\ensuremath{\mathsf{val}}[d_k]} \times \ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]}} \\
  X &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{seq}}[n]} \times \ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]}} \\
  \text{MHA}(X; W) &amp;= \left[W^O \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{head,val}}}} \text{att}(Q, K, V, M)\right]_{\ensuremath{\mathsf{seq&#39;}}\rightarrow\ensuremath{\mathsf{seq}}} \\
  Q &amp;= W^Q \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{emb}}}} \left[X\right]_{\ensuremath{\mathsf{seq}}\rightarrow\ensuremath{\mathsf{seq&#39;}}} \\
  K &amp;= W^K \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{emb}}}} X \\
  V &amp;= W^V \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{emb}}}} X \\
  M_{\ensuremath{\ensuremath{\mathsf{seq&#39;}}(i)}, \ensuremath{\ensuremath{\mathsf{seq}}[j]}} &amp;= \begin{cases} 0 &amp; j\leq i \\ -\infty &amp; \text{otherwise} \end{cases}   \end{aligned}\]</span></p>
<h4 id="layer-norm">Layer Norm</h4>
<p><span class="math display">\[\begin{aligned}
X &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]}} &amp; \gamma, \beta &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]}} \\
\text{lnorm}(X; \gamma, \beta) &amp;= \frac{X - \mathop{\mathrm{\text{mean}}}\limits_{\ensuremath{\mathsf{emb}}}(X)}{\sqrt{\mathop{\mathrm{\text{var}}}\limits_{\ensuremath{\mathsf{emb}}}(X)} + \epsilon} \odot \gamma + \beta \end{aligned}\]</span></p>
<h4 id="position-encoding">Position Encoding</h4>
<p><span class="math display">\[\begin{aligned}
X &amp;\in \{0, 1\}^{\ensuremath{\ensuremath{\mathsf{seq}}[n]} \times \ensuremath{\ensuremath{\mathsf{vocab}}[b]}} &amp; \sum\limits_{\ensuremath{\mathsf{vocab}}} X &amp;= 1\\
E &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{vocab*}}[v]} \times \ensuremath{\ensuremath{\mathsf{emb}}[d_{\text{model}}]}} \\
\text{embed}(X; E) &amp;= (E \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{vocab}}}} X)\sqrt{d_{\text{model}}} + P \\
P &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{seq}}[n]} \times \ensuremath{\ensuremath{\mathsf{hidden}}[d_{\text{model}}]}} \\
P_{\ensuremath{\ensuremath{\mathsf{hidden}}(i)}, \ensuremath{\ensuremath{\mathsf{seq}}(p)}} &amp;= \begin{cases}
  \sin((p-1) / 10000^{(i-1) / d_{\text{model}}}) &amp; \text{$i$ odd} \\ 
  \cos((p-1) / 10000^{(i-2) / d_{\text{model}}}) &amp; \text{$i$ even} \\
\end{cases} \\\end{aligned}\]</span></p>
<h4 id="transformer">Transformer</h4>
<p><span class="math display">\[\begin{aligned}
I &amp;\in \{0, 1\}^{{\ensuremath{\ensuremath{\mathsf{seq}}[n]} \times \ensuremath{\ensuremath{\mathsf{vocab}}[b]}}} &amp; \sum\limits_{\ensuremath{\mathsf{vocab}}} X &amp;= 1\\
X^0 &amp;= \text{embed}(I)\\
T^1 &amp;= \text{lnorm}(\text{MHA}(X^0)) + X^0\\
X^1 &amp;= \text{lnorm}(\text{FFN}(T^1)) + T^1\\
&amp;\vdotswithin{=} \\
T^{L} &amp;= \text{lnorm}(\text{MHA}(X^{L-1})) + X^{L-1}\\
X^{L} &amp;= \text{lnorm}(\text{FFN}(T^L)) + T^L\\
O &amp;= \mathop{\mathrm{softmax}}\limits_{\ensuremath{\mathsf{vocab}}}(W \mathbin{\mathop{\boldsymbol\cdot}\limits_{\ensuremath{\mathsf{emb}}}} X^L)\end{aligned}\]</span></p>
</body>
</html>
